\chapter{Background}\label{chap:background}
\section{State of the Art CNN Architectures}
During the past 2 decades many number of CNN architectures were designed. The first CNN architecture known as LeNet-5 was introduced in the year of 1998 by Lecun. This architecture has seven layers with 60k parameters and its main goal was to recognize hand written digits.However the network was constrained because of its inability to perform well on high resolution images.

The need for a CNN model that can also compute high resolution images lead to the development of more sophisticated architecture known as AlexNet. This was the time when neural networks gained their prominence.  The availability of GPUs and large data sets made it faster and easier to develop these kind of models. This architecture was introduced in the paper\cite{Krizhevsky} in the year 2010 which outperformed all the previous architectures. It is similar to LeNet-5 except it is more deeper with 60M parameters and with stacked convolutional layers. It has $224\times224\times3$ input layer with $11\times11 $ filter size with a stride value of 4. It was trained on Imagenet dataset which has over 15M high resolution images along with their labels.Their network topology was a bit complex because of bigger receptive fields and stride value.  According to \cite{Krizhevsky} this model achieved error rate of 17.0\%. Later on they entered a competition ILSVRC 2012 a large scale image recognition challenge and achieved a winning top error of 15.3\%. 

The next architecture was introduced by Visual Geometry Group and it is VGG16 with 16 layers. The work by \cite{SimonyanZ14a} proposed this architecture and concluded that the depth of the Convolutional Neural Network has an effect on its classification accuracy. Unlike \cite{Krizhevsky} this architecture has a very less receptive field of $3\times3$ and hence focuses on each and every part of the image. This architecture has won best classification in ILSVRC 2014. There is also VGG19 network which has 19 layers.

\noindent However as the state of the art networks keep getting deeper and deeper a new problem was discovered. Which states that when we are training a deeper architecture as the gradient is propagated to earlier layers repeated multiplication may make gradient to appear smaller or vanish totally and also accuracy gets saturated and then starts to degrade. Motivated by the thought of attacking this problem another type of network called ResNet(Residual Networks) was introduced by Kaiming He \cite{ResNet15}.This model achieved top error rate of 3.57\% and was the winner of ILSVRC 2015. Though this network is much deeper than the previous networks its complexity is lesser because they introduced a new idea of "Skip connections" it means that instead of trying to learn the direct mapping of $x$ to $H(x)$ we can calculate the residual say $F(x)=H(x)-x$ between two and then make the network to learn $F(x)+x$\cite{ResNet15}. These networks also solve vanishing gradient problem by making the gradient backpropagate through shortcuts.

ResNets totally changed the way we understand a Neural Network. This architecture paved the way to develop CNNs with hundreds of layers. These are the different type of CNN architectures that has achieve state of the art results in the field of computer vision.
\subsubsection{\textbf{\textit{Complications with the architectures}}}

Undoubtedly CNNs have almost reached the human level performance in the tasks like image recognition,classification etc.,despite of their impressive work there are still so many things that are unclear like what are the factors that contribute to the precise computations and the internal workings of the network.  Provoked by these problems lately lots of research has been done to propose different techniques that helps us visualize the internal structure of the network. Erhan et al\cite{Erhan09} made few contributions to the idea of visualizing the internal computations of the network to gain some intuitions about it. The work focused on \textbf{activation maximization}, a technique used to find the part of input image that is responsible for the units maximum activation. Usually while training a network the weights are updated iteratively but input and output image remains constant but in this technique the weights and output image are kept constant but input image is modified such that it maximizes activation maps of certain neurons. However this method was not effective in determining networks invariance.  


Next the work of Zeiler et al \cite{zeiler11} which was about reconstructing the input from the output using top down approach and in an unsupervised manner. They used the concept of deconvolution and reconstructed images at each layer with the help of filters and switch variables. These switches are located between the feature maps and pooling layers and thus they can be used during unpooling and restore the original image.  In their method they visualize each kernel by considering respective activation map and pick the image that causes the maximum activation and project it down to input space. 


Based on this work Zeiler \& Fergus et al\cite{zeiler13} developed an improvised version of deconvolution for visualizing the layers inside the network. Previously Deconvnets was used only for unsupervised data in this work they developed the \textit{Deconvnet} model for supervised data. They used multi-layered deconvolution network to project the feature maps back to the input space. Their approach had Unpooling of layers with the help of switch variables to produce feature maps and then these maps are rectified using Relu activation and then convolved with the transpose of the filters. This concept is similar to \textit{encoder-decoder}. In their method they used a pre-trained network Alexnet\cite{Krizhevsky} whose kernels were trained on \textit{ImageNet} dataset which has 1.3 million images over 1000 different classes, instead of training the network from scratch. While visualizing the architecture they were able to discover problems with the model and they changed the architecture of the first layer by reducing the filter size from $11\times 11$ to $7\times7 $ and stride to 2 in both first and second layers. Occlusion sensitivity analysis was also conducted by covering different parts of image with gray scale in order to find out which portions of the image are contributing for the classification. During visualization of internal structure of the model they also found that there exists some degree of correspondence in the network. They have conducted ablation study by removing fully connected layers and different convolutional layers and concluded that depth of the model plays a crucial role in the performance of the model. 

Next in the year of 2014 \textit{Zhou et al}\cite{Zhou14} have revolutionized the method of occluding the images \cite{zeiler13}. In this work instead of using a single gray patch for occluding the images they replicated each image by occluding many regions of it by randomized pixel values. This approach allows to analyze the important regions of an input image that are responsible for the activations produced by each unit. It also suggests that the units in the hidden layers act as object detectors of an image though there is no specification provided on the particular region of the image.

Next comes \textit{Zhou et al}\cite{Zhou15} proposed a new technique known as Class Activation Map(CAM) where an activation map is reconstructed in the previous layer by mapping the predicted class score back. This technique helps in highlighting the class specific regions which means finding the particular region in the input space that contributes more for the classification. Here the authors evaluated their work on CNNs that perform global average pooling on the convolutional feature maps just before the classification/output layer. However their work contributed more towards object localization of an image.

The other way to look in to the hidden layers of CNN was put forward by \textit{Simonyan et al}\cite{Simonyan13}. Here the authors backpropagated the class score back to input space and they proved that doing so helps in finding the pixels that are contributing most to the classification. In this way we can know if the non-linear model is looking at the main object or other parts of the image.

In \cite{Felix} the above described techniques for visualization of Convnets are classified in to three types \textit{Input Modification, Input Reconstruction } and \textit{Deconvolution}. In this work the authors developed a library called FeatureVis built on top of MatConvnet toolbox to implement and also compare the above described techniques. This library is also used in comparing different bigger architectures like GoogleNet, Vgg16 etc. All the previous have already taken the existing architectures and then visualized the internal structure of the network.

Motivated by the previous works this thesis includes visualization of the filters after training the network. It involves building and training the non-linear network from scratch instead of considering pre-existing models. This thesis is more about finding out the effect of different parameters and hyper parameters on the feature extraction part of the network. And only focused on convolution layers so densely connected layers are ignored.